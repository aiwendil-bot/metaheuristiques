Amélioration par rapport au DM1

Nous n’étions pas satisfait des temps d’exécution sur les « grosses » instances, notamment du 21-exchange.
Afin de remédier (au moins partiellement) à cela, nous avons changé de structure de données :
en effet, nous parcourrions systématiquement toute la matrice (pour repérer les mouvements possibles lors des heuristiques de recherche locale, pour vérifier si la solution était admissible, .. etc)
Le problème étant que certaines matrices sont assez creuses (de densité < 1%), il y a alors beaucoup d’informations « inutiles » puisque ce qui nous intéresse sont les liaisons variables-contraintes (les coefficients égaux à 1).
Nous avons alors créé deux fonctions permettant, à partir d’une matrice, de créer deux vecteurs de vecteurs : l’un de taille nb_variables, dont chaque élément contient les indices des contraintes liées à chaque variable, et l’autre de taille nb_contraintes,  dont chaque élément contient les indices des variables liées à chaque contrainte.
Nous avons ainsi observé une nette amélioration des temps d’exécutions 
%tableau

GRASP (Greedy Randomized Adaptative Search Procedure) est un algorithme qui fait un compromis entre glouton et aléatoire. En effet, afin d’éviter de se faire piéger dans un optimum local lors d’une recherche locale, GRASP explore plusieurs solutions de départ, qui sont construites de façon à avoir des solutions de bonne facture, tout en choisissant aléatoirement les variables mises à 1. Leur construction est la suivante : 
On contrôle le côté aléatoire avec un paramètre \alpha \in \left[ 0 ; 1 \right] (0 correspond au « tout aléatoire » et 1 au « tout glouton »)  Ce paramètre impose une certaine qualité ; pour être choisie, elle doit permettre de dépasser le seuil d’utilité 
\[
U_min + \alpha (U_max – U_min)
\]
Une variable permettant de dépasser ce seuil d’utilité est alors choisie aléatoirement, et on répète ce procédé jusqu’à ne plus pouvoir ajouter de nouvelle variable à la solution.
Une heuristique de recherche locale essaie d’améliorer la solution, puis on compare la solution avec les autres solutions de départ pour ne garder que la meilleure.

Avantages :

L’aléatoire permet de tester plusieurs solutions initiales différentes.
Le paramètre alpha permet de régler la dose d’aléatoire, afin de tester plusieurs solutions sans pour autant en obtenir d’une trop mauvaise qualité.
%Budget de calcul ? Itérations ou secondes

Exemple

Path Relinking

Le path-relinking est une méthode d’intensification de recherche de solutions dont le principe est le suivant :
On part d’une solution « d’élite » (de bonne qualité), et on crée un chemin de solutions, obtenues par un mouvement simple (flip, add, etc) ; et si une solution sur le chemin paraît prometteuse, on applique une heuristique de recherche locale. 

Notre premier choix de mise en œuvre était assez simpliste : on détermine les variables qui diffèrent entre la solution de départ et la solution cible, on effectue un flip et on regarde si la solution est admissible, puis si c’est le cas on compare avec la meilleure valeur optimale.

%faire un test pour discuter des résultats

Nous avons alors lu quelques articles pour avoir des idées sur comment exploiter au mieux le principe du path relinking

%citer papier mauricio

2 améliorations : 
ne pas prendre un flip au hasard, mais regarder celui qui améliore le plus la solution &déterminer ensuite si elle est admissible
utiliser des « bassins » de solutions d’élite (toujours dans l’idée que des bonnes solutions ont des points communs)
forward relinking 


%écrire  2 algos mauricio en latex

Le bassin de solutions d’élite est initialement vide.

A chaque itération GRASP, on construit et améliore une solution. A partir de la deuxième itération,  on applique le path-relinking à la solution qui vient d’être construite et une autre, choisie aléatoirement parmi les solutions d’élite. (nous avons choisi une loi de probabilité uniforme, mais on pourrait choisir une solution d’élite la plus « éloignée » de la solution construite.
Si le bassin n’est pas complètement rempli, on ajoute simplement la solution. Sinon, si la solution issue de l’itération GRASP est meilleure que la pire des solutions d’élite, on la remplace.

Expérimentations numériques

Bonus

Reactive grasp

De prime abord, il paraît difficile de choisir un bon, voire le meilleur alpha pour l’algorithme GRASP. La composante Reactive-Grasp permet de le choisir « automatiquement » : en effet, on intègre une phase d’apprentissage au cours des itérations, durant laquelle on choisir celui qui correspond le mieux possible aux données.
On procède ainsi :
On choisit un certain nombre de alpha possibles, et on munit cet ensemble d’une loi de probabilité uniforme. Cette probabilité sera recalculée au bout d’un certain nombre d’itérations (décidé par « l’utilisateur ») en fonction de la qualité des solutions qu’il a engendré. On recommence jusqu’à atteindre la condition d’arrêt (nombre d’itérations ou temps limite)

nouveau paramètre : N_\alpha à régler selon taille des instances ?

%tests
